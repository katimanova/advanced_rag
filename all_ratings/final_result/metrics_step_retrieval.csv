,Base rag,html_split - 3_similiatry,html_split - rerank_gigachat,html_split - rerank_gpt_3.5
faithfulness,0.8392857142857143,0.9036585365853658,0.8424494110540622,0.9161553724053724
answer_relevancy,0.7117285974977678,0.7069774803705848,0.7634337067931037,0.8539613932997934
context_precision,0.9999999998999999,0.9999999998999999,0.9999999998999999,0.9999999998999999
context_relevancy,0.7549182139699382,0.7855216622458002,0.7823607427055703,0.8031940760389037
answer_similarity,0.9239248937076107,0.9283734710452856,0.9213745107850017,0.9229667091054631
bleu_score,0.09352241379310346,0.11491379310344826,0.0781103448275862,0.015648275862068965
sim-spacy,0.8577893882349992,0.8417814785004614,0.8666157736697314,0.8935302512130058
cos-sim-TF-IDF,0.2652241379310345,0.2947380925135789,0.2956294986338564,0.27360170550604657
cos-sim-BertModel,0.9822290237931036,0.9754496356896553,0.9677166453448277,0.9776779962068964
